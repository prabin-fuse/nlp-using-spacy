{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing Using Spacy\n",
    "\n",
    "### Stop Words\n",
    "\n",
    "- Stop words are words that are filtered out before or after the natural language data(text) are processed.\n",
    "- stop words typically refers to the most common words in a language.\n",
    "- There is no universal list of stop words that is used by all NLP tools in common.\n",
    "\n",
    "**what are stop words?**\n",
    "- Stopwords are the words in any language which does not add much meaning to a sentence.\n",
    "- They can safely be ignored without sacrificing the meaning of the sentence.\n",
    "- For some search engines, these are some of the most common, short function words, such as the, is, at, which, and on.\n",
    "\n",
    "**But sometimes, stop words can be really useful and shouldnot be removed.**\n",
    "\n",
    "**When to remove stop words?**\n",
    "\n",
    "- If we have a task of text classification or sentiment analysis then we should remove stop words as they do not provide any information to our model i.e. keeping out unwanted words out of our corpus.\n",
    "- But, if we have the task of language translation then stopwords are useful, as they have to be translated along with other words.\n",
    "- There is no hard and fast rule on when to remove stop words\n",
    "    1) Remove stopwords if task to be performed is one of Language Classification, Spam Filtering, Caption Generation, Auto-Tag Generation, Sentiment analysis, or something that is related to text classification.\n",
    "    2) Better not to remove stopwords if task to be performed is one of Machine Translation, Question Answering problems, Text summarization, Language Modeling.\n",
    "\n",
    "**Pros of Removing stop words**\n",
    "\n",
    "- Stopwords are often removed from the text before training deep learning and machine learning models since stop words occur in abundance, hence providing little to no unique information that can be used for classification or clustering.\n",
    "- On removing stopwords, dataset size decreases, and the time to train the model also decreases without a huge impact on the accuracy of the model.\n",
    "- Stopword removal can potentially help in improving performance, as there are fewer and only significant tokens left. Thus, the classification accuracy could be improved.\n",
    "\n",
    "**Cons of Removing Stop Words**\n",
    "\n",
    "- Improper selection and removal of stop words can change the meaning of our text. So we have to be careful in choosing our stop words.\n",
    "- Example: This movie is not good\n",
    "    - If we remove (not ) in pre-processing step the sentence (this movie is good) indicates that it is positive which is wrongly interpreted.\n",
    "\n",
    "**Removing Stop words using SpaCy Library**\n",
    "\n",
    "- Comparing to NLTK, spacy got bigger set of stop words (326) than that of NLTK (179)\n",
    "- installation: (spacy, English Language Model)\n",
    "    - pip install -U spacy\n",
    "    - python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in c:\\users\\dell\\anaconda3\\envs\\myenvbb\\lib\\site-packages (3.7.4)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\dell\\anaconda3\\envs\\myenvbb\\lib\\site-packages (from spacy) (2.7.1)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in c:\\users\\dell\\anaconda3\\envs\\myenvbb\\lib\\site-packages (from spacy) (8.2.3)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\dell\\anaconda3\\envs\\myenvbb\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: setuptools in c:\\users\\dell\\anaconda3\\envs\\myenvbb\\lib\\site-packages (from spacy) (63.2.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\dell\\anaconda3\\envs\\myenvbb\\lib\\site-packages (from spacy) (2.31.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\dell\\anaconda3\\envs\\myenvbb\\lib\\site-packages (from spacy) (3.4.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\dell\\anaconda3\\envs\\myenvbb\\lib\\site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\dell\\anaconda3\\envs\\myenvbb\\lib\\site-packages (from spacy) (0.9.4)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\dell\\appdata\\roaming\\python\\python310\\site-packages (from spacy) (1.22.1)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\dell\\anaconda3\\envs\\myenvbb\\lib\\site-packages (from spacy) (1.1.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\dell\\appdata\\roaming\\python\\python310\\site-packages (from spacy) (4.62.3)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\dell\\anaconda3\\envs\\myenvbb\\lib\\site-packages (from spacy) (6.4.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\dell\\anaconda3\\envs\\myenvbb\\lib\\site-packages (from spacy) (3.1.4)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\dell\\anaconda3\\envs\\myenvbb\\lib\\site-packages (from spacy) (2.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\dell\\anaconda3\\envs\\myenvbb\\lib\\site-packages (from spacy) (24.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\dell\\anaconda3\\envs\\myenvbb\\lib\\site-packages (from spacy) (1.0.10)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\dell\\anaconda3\\envs\\myenvbb\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\dell\\anaconda3\\envs\\myenvbb\\lib\\site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\dell\\anaconda3\\envs\\myenvbb\\lib\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in c:\\users\\dell\\anaconda3\\envs\\myenvbb\\lib\\site-packages (from spacy) (0.3.4)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\dell\\anaconda3\\envs\\myenvbb\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\dell\\anaconda3\\envs\\myenvbb\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\dell\\anaconda3\\envs\\myenvbb\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.9.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.2 in c:\\users\\dell\\anaconda3\\envs\\myenvbb\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.18.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dell\\appdata\\roaming\\python\\python310\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\dell\\anaconda3\\envs\\myenvbb\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\dell\\appdata\\roaming\\python\\python310\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.8)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\dell\\appdata\\roaming\\python\\python310\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\dell\\anaconda3\\envs\\myenvbb\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\dell\\anaconda3\\envs\\myenvbb\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
      "Requirement already satisfied: colorama in c:\\users\\dell\\anaconda3\\envs\\myenvbb\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\dell\\anaconda3\\envs\\myenvbb\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in c:\\users\\dell\\anaconda3\\envs\\myenvbb\\lib\\site-packages (from weasel<0.4.0,>=0.1.0->spacy) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\dell\\anaconda3\\envs\\myenvbb\\lib\\site-packages (from jinja2->spacy) (2.1.2)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in c:\\users\\dell\\anaconda3\\envs\\myenvbb\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.1.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.7.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
      "     --------------------------------------- 12.8/12.8 MB 22.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.8.0,>=3.7.2 in c:\\users\\dell\\anaconda3\\envs\\myenvbb\\lib\\site-packages (from en-core-web-sm==3.7.1) (3.7.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\dell\\anaconda3\\envs\\myenvbb\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\dell\\anaconda3\\envs\\myenvbb\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in c:\\users\\dell\\anaconda3\\envs\\myenvbb\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.3)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\dell\\anaconda3\\envs\\myenvbb\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\dell\\anaconda3\\envs\\myenvbb\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\dell\\anaconda3\\envs\\myenvbb\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.4)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\dell\\anaconda3\\envs\\myenvbb\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (6.4.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\dell\\anaconda3\\envs\\myenvbb\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\dell\\anaconda3\\envs\\myenvbb\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\dell\\anaconda3\\envs\\myenvbb\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\dell\\anaconda3\\envs\\myenvbb\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.31.0)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\dell\\anaconda3\\envs\\myenvbb\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.4)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\dell\\anaconda3\\envs\\myenvbb\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\dell\\anaconda3\\envs\\myenvbb\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.7.1)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in c:\\users\\dell\\anaconda3\\envs\\myenvbb\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.3.4)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\dell\\appdata\\roaming\\python\\python310\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.62.3)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\dell\\anaconda3\\envs\\myenvbb\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\dell\\anaconda3\\envs\\myenvbb\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\dell\\anaconda3\\envs\\myenvbb\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (63.2.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\dell\\appdata\\roaming\\python\\python310\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.22.1)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\dell\\anaconda3\\envs\\myenvbb\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\dell\\anaconda3\\envs\\myenvbb\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.9.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.2 in c:\\users\\dell\\anaconda3\\envs\\myenvbb\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.18.2)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\dell\\anaconda3\\envs\\myenvbb\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.6.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\dell\\appdata\\roaming\\python\\python310\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dell\\appdata\\roaming\\python\\python310\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\dell\\appdata\\roaming\\python\\python310\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.11)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\dell\\anaconda3\\envs\\myenvbb\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\dell\\anaconda3\\envs\\myenvbb\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\dell\\anaconda3\\envs\\myenvbb\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\dell\\anaconda3\\envs\\myenvbb\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\dell\\anaconda3\\envs\\myenvbb\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in c:\\users\\dell\\anaconda3\\envs\\myenvbb\\lib\\site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\dell\\anaconda3\\envs\\myenvbb\\lib\\site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.2)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in c:\\users\\dell\\anaconda3\\envs\\myenvbb\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.1)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alone', \"'s\", 'herein', 'who', 'here', 'hereafter', 'ca', 'sometime', 'often', 'behind', 'becoming', 'though', 'bottom', 'last', 'six', 'also', 'beside', 'get', 'both', 'their', 'for', 'beyond', 'over', 'seemed', 'elsewhere', 'somewhere', 'seeming', 'others', 'the', 'anything', 'can', 'indeed', 'anyway', 'amongst', 'now', 'former', 'with', 'nothing', 'its', 'after', 'empty', 'an', 'there', 'any', 'among', 'most', 'than', 'yours', 'hence', 'his', 'ourselves', 'eight', 'four', 'done', 'hundred', 'neither', 'front', 'only', 'never', '‘re', 'various', 'something', 'doing', 'upon', \"'ll\", 'make', 'before', 'name', 'move', 'a', 'once', 'my', 'our', 'ten', 'further', 'unless', 'first', 'mostly', 'two', 'quite', 'nevertheless', 'however', 'and', 'to', 'less', 'him', 'enough', '‘d', 'during', 'well', 'five', 'amount', 'them', 'please', 'how', 'hereby', 'anyhow', 'from', 'in', 'call', 'seem', 'many', 'more', 're', 'very', 'why', 'has', \"'d\", 'that', 'too', 'wherein', 'nine', 'cannot', 'everyone', 'hereupon', 'some', 'through', 'us', 'beforehand', 'still', 'whatever', 'will', 'me', 'whose', 'ever', '’m', 'keep', 'anywhere', 'hers', 'towards', 'twelve', 'throughout', 'yourself', 'put', 'full', 'toward', 'formerly', '‘ve', 'would', 'seems', 'did', 'via', 'whole', 'thereupon', 'n’t', 'this', 'latter', 'am', 'besides', 'was', 'been', 'became', 'forty', '’ve', 'such', 'all', 'say', 'few', \"'m\", 'otherwise', 'down', 'back', 'along', 'another', 'eleven', 'have', 'of', 'up', 'since', 'show', 'everywhere', \"'ve\", 'rather', 'by', 'even', 'being', 'fifty', 'become', 'other', 'might', 'myself', 'nor', 'whereas', 'made', 'none', 'does', 'they', 'take', 'while', '’d', 'if', 'these', 'same', 'much', 'no', 'own', 'were', 'everything', 'except', 'within', 'not', 'you', 'next', 'fifteen', 'together', 'thereafter', 'n‘t', 'least', 'between', 'each', 'until', 'wherever', 'nobody', 'nowhere', 'someone', 'serious', 'third', 'are', 'where', 'using', 'perhaps', 'is', 'either', 'i', 'somehow', 'when', 'which', 'be', 'himself', 'without', 'he', 'because', 'go', \"'re\", 'should', 'every', 'again', '’re', 'sixty', 'on', 'thence', 'whether', 'may', 'moreover', 'must', 'therefore', 'whom', 'per', 'she', 'those', 'due', 'off', 'else', 'whenever', 'twenty', 'mine', 'your', '‘s', 'afterwards', 'already', 'latterly', 'almost', '‘m', 'always', 'had', 'sometimes', 'but', 'whereby', 'against', 'do', 'whoever', 'it', 'as', 'then', 'top', 'themselves', 'becomes', 'side', 'yet', 'ours', 'whence', 'several', 'itself', 'we', '’ll', 'see', 'so', 'just', 'or', 'give', 'above', 'regarding', 'anyone', 'whither', 'therein', 'thru', 'around', 'really', '’s', 'namely', 'about', 'under', 'below', 'into', 'across', \"n't\", 'yourselves', 'thereby', 'one', 'three', '‘ll', 'used', 'could', 'what', 'out', 'thus', 'whereafter', 'part', 'onto', 'meanwhile', 'whereupon', 'herself', 'her', 'although', 'noone', 'at'}\n"
     ]
    }
   ],
   "source": [
    "# list stopwords from spacy:\n",
    "stopwords = nlp.Defaults.stop_words\n",
    "print(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''Hello, this text is written from the text preprocessing with Spacy library which is open source.\n",
    "Various statistical models and pipelines are available here. Techniques like stop words,\n",
    "POS tagging and dependency matching is pre-trained.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Hello, ,, text, written, text, preprocessing, Spacy, library, open, source, ., \n",
      ", Various, statistical, models, pipelines, available, ., Techniques, like, stop, words, ,, \n",
      ", POS, tagging, dependency, matching, pre, -, trained, ., \n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# Tokenize and remove stopwords from that list of tokens.\n",
    "doc = nlp(text)\n",
    "\n",
    "# remove stopwords and make tokens without stop word.\n",
    "tokens_without_sw = [token for token in doc if token.text not in stopwords]\n",
    "print(tokens_without_sw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note : Don't forget to use `.text` attribute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization:\n",
    "- Tokenization refers to diving the whole text into multiple managable units.\n",
    "- Helps to form sequence of words or sentences.\n",
    "- Each tokens have meaning and semantic relation with other tokens.\n",
    "\n",
    "- Word Tokenization\n",
    "    - Word Tokenization simply means splitting sentence/text in words.\n",
    "    - Using attribute `token.text` to tokenize the doc\n",
    "\n",
    "- Sentence Tokenization\n",
    "\n",
    "    - Sentence Tokenization is the process of splitting up strings into sentences.\n",
    "    - A sentence usually ends with a full stop (.), here focus is to study the structure of sentence in the analysis\n",
    "    - use `sents` attribute from spacy to identify the sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'this', 'text', 'is', 'written', 'from', 'the', 'text', 'preprocessing', 'with', 'Spacy', 'library', 'which', 'is', 'open', 'source', '.', '\\n', 'Various', 'statistical', 'models', 'and', 'pipelines', 'are', 'available', 'here', '.', 'Techniques', 'like', 'stop', 'words', ',', '\\n', 'POS', 'tagging', 'and', 'dependency', 'matching', 'is', 'pre', '-', 'trained', '.', '\\n'] 45\n"
     ]
    }
   ],
   "source": [
    "# Word tokenization:\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "tokens = [token.text for token in doc]\n",
    "print(tokens, len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello, this text is written from the text preprocessing with Spacy library which is open source.\\n', 'Various statistical models and pipelines are available here.', 'Techniques like stop words,\\nPOS tagging and dependency matching is pre-trained.\\n'] 3\n"
     ]
    }
   ],
   "source": [
    "# Sentence tokenization:\n",
    "sentences = [sent.text for sent in doc.sents] \n",
    "# Or, just : list(doc.sents)\n",
    "print(sentences, len(sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Punctuation:\n",
    "- punctuation are special marks that are placed in a text to show the division between phrases and sentences.\n",
    "- There are 14 punctuation marks that are commonly used in English grammar.\n",
    "- They are, **period, question mark, exclamation point, comma, semicolon, colon, dash, hyphen, parentheses, brackets, braces, apostrophe, quotation marks, and ellipsis**.\n",
    "- We can remove punctuation from text using `is_punct` attribute.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'this', 'text', 'is', 'written', 'from', 'the', 'text', 'preprocessing', 'with', 'Spacy', 'library', 'which', 'is', 'open', 'source', '\\n', 'Various', 'statistical', 'models', 'and', 'pipelines', 'are', 'available', 'here', 'Techniques', 'like', 'stop', 'words', '\\n', 'POS', 'tagging', 'and', 'dependency', 'matching', 'is', 'pre', 'trained', '\\n']\n"
     ]
    }
   ],
   "source": [
    "# Removing the punctuation:\n",
    "# Detecting each token and only appending those tokens which is not punct:\n",
    "\n",
    "non_punct_tokens = []\n",
    "for token in doc:\n",
    "    if token.is_punct == False:\n",
    "        non_punct_tokens.append(token.text)\n",
    "\n",
    "print(non_punct_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lower Casing:\n",
    "- Converting word to lower case (NLP->nlp).\n",
    "- **Q.Why Lower Casing?**\n",
    "    - Words like Book and book mean the same,\n",
    "    - When not converted to the lower case those two are represented as two different words in the vector space model (resulting in more dimension).\n",
    "    - Higher the dimension, more computation resources are required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello, this text is written from the text preprocessing with spacy library which is open source.\\nvarious statistical models and pipelines are available here. techniques like stop words,\\npos tagging and dependency matching is pre-trained.\\n'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lower_text = text.lower()\n",
    "lower_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming:\n",
    "- Converting to the words or tokens to their root word. \n",
    "- The root word might not make sense\n",
    "- It is based on algorithm\n",
    "\n",
    "Note: Stemming is not available in Spacy library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization:\n",
    "- Lemmatization is the process of converting a word to its base form.\n",
    "- For example, lemmatization would correctly identify the base form of caring to care\n",
    "- Lemmatization can be carried out using the attribute `token.lemma_`\n",
    "- It is search-based algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello => hello\n",
      ", => ,\n",
      "this => this\n",
      "text => text\n",
      "is => be\n",
      "written => write\n",
      "from => from\n",
      "the => the\n",
      "text => text\n",
      "preprocessing => preprocesse\n",
      "with => with\n",
      "Spacy => Spacy\n",
      "library => library\n",
      "which => which\n",
      "is => be\n",
      "open => open\n",
      "source => source\n",
      ". => .\n",
      "\n",
      " => \n",
      "\n",
      "Various => various\n",
      "statistical => statistical\n",
      "models => model\n",
      "and => and\n",
      "pipelines => pipeline\n",
      "are => be\n",
      "available => available\n",
      "here => here\n",
      ". => .\n",
      "Techniques => technique\n",
      "like => like\n",
      "stop => stop\n",
      "words => word\n",
      ", => ,\n",
      "\n",
      " => \n",
      "\n",
      "POS => POS\n",
      "tagging => tagging\n",
      "and => and\n",
      "dependency => dependency\n",
      "matching => matching\n",
      "is => be\n",
      "pre => pre\n",
      "- => -\n",
      "trained => train\n",
      ". => .\n",
      "\n",
      " => \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text, \"=>\", token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS Tagging:\n",
    "- Parts-of-speech tagging is the process of tagging words in textual input with their appropriate parts of speech.\n",
    "- This is one of the core feature loaded into the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenvbb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
