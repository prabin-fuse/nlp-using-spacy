{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing Using Spacy\n",
    "\n",
    "### Stop Words\n",
    "\n",
    "- Stop words are words that are filtered out before or after the natural language data(text) are processed.\n",
    "- stop words typically refers to the most common words in a language.\n",
    "- There is no universal list of stop words that is used by all NLP tools in common.\n",
    "\n",
    "**what are stop words?**\n",
    "- Stopwords are the words in any language which does not add much meaning to a sentence.\n",
    "- They can safely be ignored without sacrificing the meaning of the sentence.\n",
    "- For some search engines, these are some of the most common, short function words, such as the, is, at, which, and on.\n",
    "\n",
    "**But sometimes, stop words can be really useful and shouldnot be removed.**\n",
    "\n",
    "**When to remove stop words?**\n",
    "\n",
    "- If we have a task of text classification or sentiment analysis then we should remove stop words as they do not provide any information to our model i.e. keeping out unwanted words out of our corpus.\n",
    "- But, if we have the task of language translation then stopwords are useful, as they have to be translated along with other words.\n",
    "- There is no hard and fast rule on when to remove stop words\n",
    "    1) Remove stopwords if task to be performed is one of Language Classification, Spam Filtering, Caption Generation, Auto-Tag Generation, Sentiment analysis, or something that is related to text classification.\n",
    "    2) Better not to remove stopwords if task to be performed is one of Machine Translation, Question Answering problems, Text summarization, Language Modeling.\n",
    "\n",
    "**Pros of Removing stop words**\n",
    "\n",
    "- Stopwords are often removed from the text before training deep learning and machine learning models since stop words occur in abundance, hence providing little to no unique information that can be used for classification or clustering.\n",
    "- On removing stopwords, dataset size decreases, and the time to train the model also decreases without a huge impact on the accuracy of the model.\n",
    "- Stopword removal can potentially help in improving performance, as there are fewer and only significant tokens left. Thus, the classification accuracy could be improved.\n",
    "\n",
    "**Cons of Removing Stop Words**\n",
    "\n",
    "- Improper selection and removal of stop words can change the meaning of our text. So we have to be careful in choosing our stop words.\n",
    "- Example: This movie is not good\n",
    "    - If we remove (not ) in pre-processing step the sentence (this movie is good) indicates that it is positive which is wrongly interpreted.\n",
    "\n",
    "**Removing Stop words using SpaCy Library**\n",
    "\n",
    "- Comparing to NLTK, spacy got bigger set of stop words (326) than that of NLTK (179)\n",
    "- installation: (spacy, English Language Model)\n",
    "    - pip install -U spacy\n",
    "    - python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in c:\\users\\dell\\anaconda3\\envs\\myenvbb\\lib\\site-packages (3.7.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\dell\\anaconda3\\envs\\myenvbb\\lib\\site-packages (from spacy) (1.0.10)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\dell\\anaconda3\\envs\\myenvbb\\lib\\site-packages (from spacy) (0.9.4)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\dell\\anaconda3\\envs\\myenvbb\\lib\\site-packages (from spacy) (3.4.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\dell\\anaconda3\\envs\\myenvbb\\lib\\site-packages (from spacy) (6.4.0)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\dell\\anaconda3\\envs\\myenvbb\\lib\\site-packages (from spacy) (2.4.8)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\dell\\anaconda3\\envs\\myenvbb\\lib\\site-packages (from spacy) (1.1.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\dell\\anaconda3\\envs\\myenvbb\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\dell\\anaconda3\\envs\\myenvbb\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\dell\\appdata\\roaming\\python\\python310\\site-packages (from spacy) (4.62.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\dell\\anaconda3\\envs\\myenvbb\\lib\\site-packages (from spacy) (63.2.0)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in c:\\users\\dell\\anaconda3\\envs\\myenvbb\\lib\\site-packages (from spacy) (0.3.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\dell\\anaconda3\\envs\\myenvbb\\lib\\site-packages (from spacy) (24.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\dell\\anaconda3\\envs\\myenvbb\\lib\\site-packages (from spacy) (2.31.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\dell\\anaconda3\\envs\\myenvbb\\lib\\site-packages (from spacy) (3.1.4)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\dell\\anaconda3\\envs\\myenvbb\\lib\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\dell\\anaconda3\\envs\\myenvbb\\lib\\site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\dell\\appdata\\roaming\\python\\python310\\site-packages (from spacy) (1.22.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\dell\\anaconda3\\envs\\myenvbb\\lib\\site-packages (from spacy) (2.7.1)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in c:\\users\\dell\\anaconda3\\envs\\myenvbb\\lib\\site-packages (from spacy) (8.2.3)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\dell\\anaconda3\\envs\\myenvbb\\lib\\site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\dell\\anaconda3\\envs\\myenvbb\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\dell\\anaconda3\\envs\\myenvbb\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.9.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.2 in c:\\users\\dell\\anaconda3\\envs\\myenvbb\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.18.2)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\dell\\anaconda3\\envs\\myenvbb\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\dell\\appdata\\roaming\\python\\python310\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\dell\\anaconda3\\envs\\myenvbb\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\dell\\appdata\\roaming\\python\\python310\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dell\\appdata\\roaming\\python\\python310\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.10.8)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\dell\\anaconda3\\envs\\myenvbb\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\dell\\anaconda3\\envs\\myenvbb\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\dell\\anaconda3\\envs\\myenvbb\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\dell\\anaconda3\\envs\\myenvbb\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in c:\\users\\dell\\anaconda3\\envs\\myenvbb\\lib\\site-packages (from weasel<0.4.0,>=0.1.0->spacy) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\dell\\anaconda3\\envs\\myenvbb\\lib\\site-packages (from jinja2->spacy) (2.1.2)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in c:\\users\\dell\\anaconda3\\envs\\myenvbb\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.1.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.7.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 12.8/12.8 MB 5.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.8.0,>=3.7.2 in c:\\users\\dell\\anaconda3\\envs\\myenvbb\\lib\\site-packages (from en-core-web-sm==3.7.1) (3.7.4)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\dell\\anaconda3\\envs\\myenvbb\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.7.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\dell\\anaconda3\\envs\\myenvbb\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\dell\\anaconda3\\envs\\myenvbb\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (63.2.0)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\dell\\anaconda3\\envs\\myenvbb\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\dell\\anaconda3\\envs\\myenvbb\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\dell\\anaconda3\\envs\\myenvbb\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (6.4.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\dell\\anaconda3\\envs\\myenvbb\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.31.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\dell\\anaconda3\\envs\\myenvbb\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\dell\\anaconda3\\envs\\myenvbb\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\dell\\anaconda3\\envs\\myenvbb\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\dell\\anaconda3\\envs\\myenvbb\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\dell\\appdata\\roaming\\python\\python310\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.62.3)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\dell\\appdata\\roaming\\python\\python310\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.22.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\dell\\anaconda3\\envs\\myenvbb\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\dell\\anaconda3\\envs\\myenvbb\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.0)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\dell\\anaconda3\\envs\\myenvbb\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in c:\\users\\dell\\anaconda3\\envs\\myenvbb\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.3.4)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in c:\\users\\dell\\anaconda3\\envs\\myenvbb\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.3)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\dell\\anaconda3\\envs\\myenvbb\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\dell\\anaconda3\\envs\\myenvbb\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\dell\\anaconda3\\envs\\myenvbb\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.2 in c:\\users\\dell\\anaconda3\\envs\\myenvbb\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.18.2)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\dell\\anaconda3\\envs\\myenvbb\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.9.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\dell\\anaconda3\\envs\\myenvbb\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.6.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\dell\\appdata\\roaming\\python\\python310\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dell\\appdata\\roaming\\python\\python310\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\dell\\anaconda3\\envs\\myenvbb\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\dell\\appdata\\roaming\\python\\python310\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\dell\\anaconda3\\envs\\myenvbb\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\dell\\anaconda3\\envs\\myenvbb\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
      "Requirement already satisfied: colorama in c:\\users\\dell\\anaconda3\\envs\\myenvbb\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\dell\\anaconda3\\envs\\myenvbb\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in c:\\users\\dell\\anaconda3\\envs\\myenvbb\\lib\\site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\dell\\anaconda3\\envs\\myenvbb\\lib\\site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.2)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in c:\\users\\dell\\anaconda3\\envs\\myenvbb\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.1)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'our', 'than', 'whereas', 'would', 'there', 'being', 'not', 'take', 'except', 'were', 'least', 'afterwards', 'sometimes', 'much', 'among', 'part', 'regarding', 'must', 'others', 'elsewhere', 'indeed', 'off', 'now', 'throughout', 'top', '‘ll', 'yet', 'whether', 'whereby', 'hundred', 'herself', 'all', 'made', 'whoever', 'whither', 'unless', 'other', '‘re', 'twenty', 'name', 'get', 'one', 'someone', 'ever', 'had', 'below', '’m', 'nine', 'is', 'though', 'most', '‘ve', 'whereupon', 'before', 'keep', 'seemed', 'never', 'many', \"'d\", 'same', 'ours', 'hence', 'for', 'doing', '‘s', 'beforehand', \"n't\", 'so', 'are', 'whom', 'hers', 'was', 'did', 'seeming', 'eight', 'please', 'ten', 'two', 'might', 'noone', 'nowhere', 'it', 'already', 'these', 'due', 'hereby', 'in', 'no', 'somehow', 'within', 'a', 'been', 'that', 'anything', 'to', 'through', 'done', 'twelve', 'mine', 'five', 'often', 'from', 'thence', 'why', 'each', 'without', 'thereby', 'anywhere', 'formerly', 'perhaps', 'just', 'seem', 'seems', 'however', 'could', 'some', 'against', 'whatever', 'whole', 'over', 'who', 'how', 'almost', 'either', 'thru', 'themselves', 'since', 'because', 'he', '’s', 'whose', 'such', 'enough', 'its', 'together', 'three', 'up', 'everything', '’ll', 'us', 'behind', 'became', 'as', 'else', 'the', 'with', 'side', 'an', 'between', 'move', 'both', '‘m', 'very', 'per', 'again', 'may', 'nobody', 'fifteen', 'still', 'any', \"'m\", 'six', 'your', 'out', 'various', 'go', 'anyhow', 'besides', 'which', \"'ve\", 'she', 'under', 'if', 'more', 'into', 'something', 'upon', 'moreover', 'i', 'his', 'thereafter', 'another', 'yourself', '‘d', 'front', 'nevertheless', 'give', 'less', 'really', 'latter', 'they', 'next', 'otherwise', 'quite', 'forty', 'wherein', 'n‘t', 'using', 'back', \"'ll\", 'yours', 'what', 'whereafter', 'my', 'amongst', \"'s\", 'several', 'after', 'therefore', 'bottom', 'four', 'am', 'can', 'last', 'while', 'anyway', 'and', 'ourselves', 'by', 'eleven', 'will', 'via', 'across', 'become', 'every', 'their', 'of', 'former', 'show', 'then', 'hereupon', 'none', 'around', 'toward', 'further', 'but', 'about', 'own', 'put', 're', 'be', 'everywhere', 'mostly', 'has', \"'re\", 'those', 'make', 'neither', 'too', 'here', 'this', 'you', 'well', 'third', 'along', 'everyone', 'see', 'should', 'down', 'on', 'where', 'beside', 'have', 'beyond', 'meanwhile', 'few', 'do', 'first', 'even', 'full', 'above', 'them', 'sixty', 'serious', 'rather', 'also', 'nothing', 'until', 'does', 'whence', 'only', 'always', 'namely', 'hereafter', 'or', 'nor', 'wherever', 'him', 'fifty', 'although', '’d', 'during', 'myself', 'ca', 'latterly', 'cannot', 'say', 'anyone', 'becoming', 'amount', 'when', 'we', 'becomes', 'once', 'towards', '’re', 'onto', 'itself', 'thereupon', 'n’t', 'himself', 'her', 'somewhere', 'herein', '’ve', 'call', 'thus', 'yourselves', 'me', 'at', 'empty', 'used', 'whenever', 'alone', 'therein', 'sometime'}\n"
     ]
    }
   ],
   "source": [
    "# list stopwords from spacy:\n",
    "stopwords = nlp.Defaults.stop_words\n",
    "print(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''Hello, this text is written from the text preprocessing with Spacy library which is open source.\n",
    "Various statistical models and pipelines are available here. Techniques like stop words,\n",
    "POS tagging and dependency matching is pre-trained.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Hello, ,, text, written, text, preprocessing, Spacy, library, open, source, ., \n",
      ", Various, statistical, models, pipelines, available, ., Techniques, like, stop, words, ,, \n",
      ", POS, tagging, dependency, matching, pre, -, trained, ., \n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# Tokenize and remove stopwords from that list of tokens.\n",
    "doc = nlp(text)\n",
    "\n",
    "# remove stopwords and make tokens without stop word.\n",
    "tokens_without_sw = [token for token in doc if token.text not in stopwords]\n",
    "print(tokens_without_sw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note : Don't forget to use `.text` attribute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization:\n",
    "- Tokenization refers to diving the whole text into multiple managable units.\n",
    "- Helps to form sequence of words or sentences.\n",
    "- Each tokens have meaning and semantic relation with other tokens.\n",
    "\n",
    "- Word Tokenization\n",
    "    - Word Tokenization simply means splitting sentence/text in words.\n",
    "    - Using attribute `token.text` to tokenize the doc\n",
    "\n",
    "- Sentence Tokenization\n",
    "\n",
    "    - Sentence Tokenization is the process of splitting up strings into sentences.\n",
    "    - A sentence usually ends with a full stop (.), here focus is to study the structure of sentence in the analysis\n",
    "    - use `sents` attribute from spacy to identify the sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'this', 'text', 'is', 'written', 'from', 'the', 'text', 'preprocessing', 'with', 'Spacy', 'library', 'which', 'is', 'open', 'source', '.', '\\n', 'Various', 'statistical', 'models', 'and', 'pipelines', 'are', 'available', 'here', '.', 'Techniques', 'like', 'stop', 'words', ',', '\\n', 'POS', 'tagging', 'and', 'dependency', 'matching', 'is', 'pre', '-', 'trained', '.', '\\n'] 45\n"
     ]
    }
   ],
   "source": [
    "# Word tokenization:\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "tokens = [token.text for token in doc]\n",
    "print(tokens, len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello, this text is written from the text preprocessing with Spacy library which is open source.\\n', 'Various statistical models and pipelines are available here.', 'Techniques like stop words,\\nPOS tagging and dependency matching is pre-trained.\\n'] 3\n"
     ]
    }
   ],
   "source": [
    "# Sentence tokenization:\n",
    "sentences = [sent.text for sent in doc.sents] \n",
    "# Or, just : list(doc.sents)\n",
    "print(sentences, len(sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Punctuation:\n",
    "- punctuation are special marks that are placed in a text to show the division between phrases and sentences.\n",
    "- There are 14 punctuation marks that are commonly used in English grammar.\n",
    "- They are, **period, question mark, exclamation point, comma, semicolon, colon, dash, hyphen, parentheses, brackets, braces, apostrophe, quotation marks, and ellipsis**.\n",
    "- We can remove punctuation from text using `is_punct` attribute.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'this', 'text', 'is', 'written', 'from', 'the', 'text', 'preprocessing', 'with', 'Spacy', 'library', 'which', 'is', 'open', 'source', '\\n', 'Various', 'statistical', 'models', 'and', 'pipelines', 'are', 'available', 'here', 'Techniques', 'like', 'stop', 'words', '\\n', 'POS', 'tagging', 'and', 'dependency', 'matching', 'is', 'pre', 'trained', '\\n']\n"
     ]
    }
   ],
   "source": [
    "# Removing the punctuation:\n",
    "# Detecting each token and only appending those tokens which is not punct:\n",
    "\n",
    "non_punct_tokens = []\n",
    "for token in doc:\n",
    "    if token.is_punct == False:\n",
    "        non_punct_tokens.append(token.text)\n",
    "\n",
    "print(non_punct_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lower Casing:\n",
    "- Converting word to lower case (NLP->nlp).\n",
    "- **Q.Why Lower Casing?**\n",
    "    - Words like Book and book mean the same,\n",
    "    - When not converted to the lower case those two are represented as two different words in the vector space model (resulting in more dimension).\n",
    "    - Higher the dimension, more computation resources are required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello, this text is written from the text preprocessing with spacy library which is open source.\\nvarious statistical models and pipelines are available here. techniques like stop words,\\npos tagging and dependency matching is pre-trained.\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lower_text = text.lower()\n",
    "lower_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming:\n",
    "- Converting to the words or tokens to their root word. \n",
    "- The root word might not make sense\n",
    "- It is based on algorithm\n",
    "\n",
    "Note: Stemming is not available in Spacy library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization:\n",
    "- Lemmatization is the process of converting a word to its base form.\n",
    "- For example, lemmatization would correctly identify the base form of caring to care\n",
    "- Lemmatization can be carried out using the attribute `token.lemma_`\n",
    "- It is search-based algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello => hello\n",
      ", => ,\n",
      "this => this\n",
      "text => text\n",
      "is => be\n",
      "written => write\n",
      "from => from\n",
      "the => the\n",
      "text => text\n",
      "preprocessing => preprocesse\n",
      "with => with\n",
      "Spacy => Spacy\n",
      "library => library\n",
      "which => which\n",
      "is => be\n",
      "open => open\n",
      "source => source\n",
      ". => .\n",
      "\n",
      " => \n",
      "\n",
      "Various => various\n",
      "statistical => statistical\n",
      "models => model\n",
      "and => and\n",
      "pipelines => pipeline\n",
      "are => be\n",
      "available => available\n",
      "here => here\n",
      ". => .\n",
      "Techniques => technique\n",
      "like => like\n",
      "stop => stop\n",
      "words => word\n",
      ", => ,\n",
      "\n",
      " => \n",
      "\n",
      "POS => POS\n",
      "tagging => tagging\n",
      "and => and\n",
      "dependency => dependency\n",
      "matching => matching\n",
      "is => be\n",
      "pre => pre\n",
      "- => -\n",
      "trained => train\n",
      ". => .\n",
      "\n",
      " => \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text, \"=>\", token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS Tagging:\n",
    "- Parts-of-speech tagging is the process of tagging words in textual input with their appropriate parts of speech.\n",
    "- This is one of the core feature loaded into the pipeline.\n",
    "- POS tag can be accessed using `token.pos_`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello INTJ UH\n",
      ", PUNCT ,\n",
      "this DET DT\n",
      "text NOUN NN\n",
      "is AUX VBZ\n",
      "written VERB VBN\n",
      "from ADP IN\n",
      "the DET DT\n",
      "text NOUN NN\n",
      "preprocessing VERB VBG\n",
      "with ADP IN\n",
      "Spacy PROPN NNP\n",
      "library NOUN NN\n",
      "which PRON WDT\n",
      "is AUX VBZ\n",
      "open ADJ JJ\n",
      "source NOUN NN\n",
      ". PUNCT .\n",
      "\n",
      " SPACE _SP\n",
      "Various ADJ JJ\n",
      "statistical ADJ JJ\n",
      "models NOUN NNS\n",
      "and CCONJ CC\n",
      "pipelines NOUN NNS\n",
      "are AUX VBP\n",
      "available ADJ JJ\n",
      "here ADV RB\n",
      ". PUNCT .\n",
      "Techniques NOUN NNS\n",
      "like AUX IN\n",
      "stop VERB VB\n",
      "words NOUN NNS\n",
      ", PUNCT ,\n",
      "\n",
      " SPACE _SP\n",
      "POS PROPN NNP\n",
      "tagging NOUN NN\n",
      "and CCONJ CC\n",
      "dependency NOUN NN\n",
      "matching NOUN NN\n",
      "is AUX VBZ\n",
      "pre VERB VBN\n",
      "- ADJ JJ\n",
      "trained VERB VBN\n",
      ". PUNCT .\n",
      "\n",
      " SPACE _SP\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.tag_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Recognition\n",
    "- It is the process of detecting the named entities such as the person name, the location name, the company name, the quantities and the monetary value.\n",
    "- We can find the named entity using spaCy `ents` attribute class.\n",
    "- `entity.text` and `entity.label`\n",
    "- Entity attributes details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "Albert Einstein was a German-born theoretical physicist who developed the theory of relativity, one of the two pillars of modern physics. His work is also known for its influence on the philosophy of science. He was born in Ulm, in the Kingdom of Württemberg in the German Empire, on 14 March 1879. When he was 17, he moved to Switzerland, where he began his theoretical physics studies at the Swiss Federal Institute of Technology in Zurich. He published his first paper in 1900, at the age of 21.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Albert Einstein PERSON\n",
      "German NORP\n",
      "one CARDINAL\n",
      "two CARDINAL\n",
      "Ulm GPE\n",
      "the Kingdom of Württemberg GPE\n",
      "the German Empire GPE\n",
      "14 March 1879 DATE\n",
      "17 DATE\n",
      "Switzerland GPE\n",
      "the Swiss Federal Institute of Technology ORG\n",
      "Zurich GPE\n",
      "first ORDINAL\n",
      "1900 DATE\n",
      "the age of 21 DATE\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "from spacy import displacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "for entity in doc.ents:\n",
    "    print(entity.text, entity.label_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dell\\anaconda3\\envs\\myenvbb\\lib\\site-packages\\spacy\\displacy\\__init__.py:106: UserWarning: [W011] It looks like you're calling displacy.serve from within a Jupyter notebook or a similar environment. This likely means you're already running a local web server, so there's no need to make displaCy start another one. Instead, you should be able to replace displacy.serve with displacy.render to show the visualization.\n",
      "  warnings.warn(Warnings.W011)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><!DOCTYPE html>\n",
       "<html lang=\"en\">\n",
       "    <head>\n",
       "        <title>displaCy</title>\n",
       "    </head>\n",
       "\n",
       "    <body style=\"font-size: 16px; font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol'; padding: 4rem 2rem; direction: ltr\">\n",
       "<figure style=\"margin-bottom: 6rem\">\n",
       "<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\"><br>\n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Albert Einstein\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " was a \n",
       "<mark class=\"entity\" style=\"background: #c887fb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    German\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">NORP</span>\n",
       "</mark>\n",
       "-born theoretical physicist who developed the theory of relativity, \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    one\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       " of the \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    two\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       " pillars of modern physics. His work is also known for its influence on the philosophy of science. He was born in \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Ulm\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       ", in \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    the Kingdom of Württemberg\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " in \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    the German Empire\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       ", on \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    14 March 1879\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       ". When he was \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    17\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       ", he moved to \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Switzerland\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       ", where he began his theoretical physics studies at \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    the Swiss Federal Institute of Technology\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " in \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Zurich\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       ". He published his \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    first\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORDINAL</span>\n",
       "</mark>\n",
       " paper in \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    1900\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       ", at \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    the age of 21\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       ".<br></div>\n",
       "</figure>\n",
       "</body>\n",
       "</html></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using the 'ent' visualizer\n",
      "Serving on http://0.0.0.0:5000 ...\n",
      "\n",
      "Shutting down server on port 5000.\n"
     ]
    }
   ],
   "source": [
    "displacy.serve(doc, style = \"ent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenvbb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
